{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/niit-ibm/lt4-pe-lab1/blob/main/sim1_handson.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTqBIB7Omd93"
      },
      "source": [
        "# LAB 1: Prompt Engineering for Project Status Updates\n",
        "## Hands-On Exercise Using IBM Granite Model via Replicate\n",
        "\n",
        "### Scenario\n",
        "You are a project intern at a mid-sized IT services company, supporting several project teams. You draft 120-word status updates for internal and external stakeholders weekly. Project leads send information in different formats, making AI-generated drafts inconsistent.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t00Gj-BNmd95"
      },
      "source": [
        "## Setup: Install Required Libraries\n",
        "\n",
        "First, install the necessary libraries as used in the reference implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "743I23kvmd95"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install \"langchain_community<0.3.0\" replicate ipywidgets ibm-granite-community --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHyoRz6vmd96"
      },
      "source": [
        "## Configure Environment and Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "17ZcM1y-md96"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import replicate\n",
        "from langchain_community.llms import Replicate\n",
        "from ibm_granite_community.notebook_utils import get_env_var\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown, HTML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-SWRljomd97"
      },
      "source": [
        "## Set Replicate API Token\n",
        "\n",
        "You need a Replicate API token to use the IBM Granite model. Get your token from [replicate.com](https://replicate.com/account/api-tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gUVkPNbmd97"
      },
      "outputs": [],
      "source": [
        "# Use the utility function to get from environment or prompt\n",
        "replicate_api_token = get_env_var(\"REPLICATE_API_TOKEN\")\n",
        "os.environ[\"REPLICATE_API_TOKEN\"] = replicate_api_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZWvaF24md97"
      },
      "source": [
        "---\n",
        "## Task 1: Initialize the IBM Granite Foundation Model\n",
        "\n",
        "We'll use the **IBM Granite 3.3 8B Instruct** model via Replicate, matching the reference implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77Z9KDjrmd97"
      },
      "outputs": [],
      "source": [
        "# Model configuration - exactly as in reference\n",
        "MODEL_NAME = \"ibm-granite/granite-3.3-8b-instruct\"\n",
        "MAX_TOKENS = 1024\n",
        "TEMPERATURE = 0.2\n",
        "\n",
        "# Initialize the model\n",
        "llm = Replicate(\n",
        "    model=MODEL_NAME,\n",
        "    model_kwargs={\n",
        "        \"max_tokens\": MAX_TOKENS,\n",
        "        \"temperature\": TEMPERATURE\n",
        "    }\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Model initialized: {MODEL_NAME}\")\n",
        "print(f\"   Max Tokens: {MAX_TOKENS}\")\n",
        "print(f\"   Temperature: {TEMPERATURE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZlI_3RZmd97"
      },
      "source": [
        "## Sample Project Input Data\n",
        "\n",
        "Here are two different formats of project updates you might receive from project leads:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqU1-2TImd98"
      },
      "outputs": [],
      "source": [
        "# Input A: Bulleted, messy, partial notes\n",
        "input_a = \"\"\"Project: Alpha CRM Upgrade ‚Äî Weekly Update Notes\n",
        "‚Ä¢ Login issue fixed for 80% users, pending bug #3421\n",
        "‚Ä¢ Deployment for Phase 2 pushed from 15th to 19th\n",
        "‚Ä¢ Client appreciated responsiveness\n",
        "‚Ä¢ Risk: insufficient test data for UAT\n",
        "‚Ä¢ Next steps: Dev team to finalize API changes, Testing team to complete regression\n",
        "‚Ä¢ Need to highlight dependency on Data Engineering\n",
        "\"\"\"\n",
        "\n",
        "# Input B: Narrative-style email\n",
        "input_b = \"\"\"Forwarded message from Priya (Project Lead ‚Äî Employee Portal Revamp)\n",
        "Here's the update you asked for ‚Äî We completed the UI redesign for the feedback module and shared it with the client yesterday. They're happy with the look and want us to proceed. We're still waiting for HR to confirm the final list of features for Phase 2, so timelines may shift a bit. Testing is ongoing. No major blockers right now, but remind the team that the API documentation still needs review. Let's capture that as a key reminder.\n",
        "\"\"\"\n",
        "\n",
        "print(\"Sample inputs loaded successfully!\")\n",
        "print(\"\\nYou can use 'input_a' or 'input_b' in the exercises below.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnaFyySLmd98"
      },
      "source": [
        "---\n",
        "## Task 2: Start with a Direct Prompt\n",
        "\n",
        "### Teaching Moment: Direct Prompts\n",
        "\n",
        "A ** Direct Prompt** provides minimal guidance to the model. Let's see what happens when we use a simple, direct prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxf2TH3Cmd98"
      },
      "outputs": [],
      "source": [
        "# Direct, zero-shot prompt\n",
        "zero_shot_prompt = \"Write a 120-word weekly project update.\"\n",
        "\n",
        "print(\"üîµ TASK 2: Zero-Shot Prompting\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Prompt: {zero_shot_prompt}\\n\")\n",
        "\n",
        "# Get response from model\n",
        "zero_shot_response = llm.invoke(zero_shot_prompt)\n",
        "\n",
        "print(\"Response:\")\n",
        "print(\"-\"*60)\n",
        "print(zero_shot_response)\n",
        "print(\"-\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAXHANcLmd98"
      },
      "source": [
        "### Task 2: Analysis\n",
        "\n",
        "**Observe the limitations:**\n",
        "- Output is vague and generic\n",
        "- No specific structure\n",
        "- May repeat wording from the input\n",
        "- Lacks context about the project\n",
        "\n",
        "**Conclusion:** A direct, zero-shot prompt is insufficient for generating structured, reliable project updates. We need to add more guidance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7REKvw-Omd98"
      },
      "source": [
        "---\n",
        "## Task 3: Refine to a Task-Specific Prompt with Structured Instructions\n",
        "\n",
        "Now let's add:\n",
        "- Clear task description\n",
        "- Context about the audience\n",
        "- Desired output format\n",
        "\n",
        "We'll use one of the sample inputs to make it more realistic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpbMvwv7md99"
      },
      "outputs": [],
      "source": [
        "# Task-specific prompt with structure\n",
        "def create_structured_prompt(project_input):\n",
        "    prompt = f\"\"\"Create a 120-word weekly project update for internal stakeholders.\n",
        "Include: current status, completed tasks, risks, and next steps.\n",
        "\n",
        "Based on this input:\n",
        "{project_input}\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n",
        "# Let's use Input A for this task\n",
        "structured_prompt = create_structured_prompt(input_a)\n",
        "\n",
        "print(\"üü¢ TASK 3: Task-Specific Prompt with Structured Instructions\")\n",
        "print(\"=\"*60)\n",
        "print(\"Prompt:\")\n",
        "print(structured_prompt)\n",
        "print(\"\\nGenerating response...\\n\")\n",
        "\n",
        "# Get response from model\n",
        "structured_response = llm.invoke(structured_prompt)\n",
        "\n",
        "print(\"Response:\")\n",
        "print(\"-\"*60)\n",
        "print(structured_response)\n",
        "print(\"-\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSMH78LXmd99"
      },
      "source": [
        "### Task 3: Analysis\n",
        "\n",
        "**Improvements observed:**\n",
        "- Output becomes clearer and more focused\n",
        "- More relevant information appears\n",
        "- Better structure emerges\n",
        "\n",
        "**Remaining issues:**\n",
        "- May not match the professional tone expected\n",
        "- Format could be more consistent\n",
        "\n",
        "**Conclusion:** Task-specific prompts with structured instructions improve output quality, but we need to refine further by adding role, tone, and format specifications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVK2c2bMmd99"
      },
      "source": [
        "---\n",
        "## Task 4: Refine Using Advanced Prompt Engineering Techniques\n",
        "\n",
        "Now we'll apply best practices:\n",
        "- **Role prompting**: Define who the AI is\n",
        "- **Tone specification**: Set the communication style\n",
        "- **Format constraints**: Specify exact output format\n",
        "\n",
        "This creates a comprehensive, well-engineered prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjfcZa9Gmd99"
      },
      "outputs": [],
      "source": [
        "# Advanced prompt with role, tone, and format\n",
        "def create_advanced_prompt(project_input):\n",
        "    prompt = f\"\"\"You are a project reporting assistant for an IT services company.\n",
        "\n",
        "Task: Generate a 120-word weekly project update for internal stakeholders.\n",
        "\n",
        "Tone: Concise and professional.\n",
        "\n",
        "Format: Use bullet points for the following sections:\n",
        "- Current Status\n",
        "- Completed Tasks\n",
        "- Risks/Issues\n",
        "- Next Steps\n",
        "\n",
        "Input information:\n",
        "{project_input}\n",
        "\n",
        "Generate the update now:\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n",
        "# Test with Input A\n",
        "advanced_prompt = create_advanced_prompt(input_a)\n",
        "\n",
        "print(\"üü£ TASK 4: Advanced Prompt Engineering\")\n",
        "print(\"=\"*60)\n",
        "print(\"Prompt:\")\n",
        "print(advanced_prompt)\n",
        "print(\"\\nGenerating response...\\n\")\n",
        "\n",
        "# Get response from model\n",
        "advanced_response = llm.invoke(advanced_prompt)\n",
        "\n",
        "print(\"Response:\")\n",
        "print(\"-\"*60)\n",
        "print(advanced_response)\n",
        "print(\"-\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tk7q2wxomd99"
      },
      "source": [
        "### Task 4: Analysis\n",
        "\n",
        "**Results:**\n",
        "- Highly structured output\n",
        "- Professional tone that matches workplace expectations\n",
        "- No irrelevant details or repetition\n",
        "- Consistent format across different inputs\n",
        "\n",
        "**Conclusion:** By combining task description, context, output format, role, and tone, you produce polished, stakeholder-ready project updates. This demonstrates how layering multiple prompting techniques leads to high-quality outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJd642Pnmd99"
      },
      "source": [
        "---\n",
        "## Comparison: All Three Approaches Side by Side"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "eBoX_5Vcmd99",
        "outputId": "cad148b3-00e5-4484-95e1-c6a786750155"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "        .comparison-container {\n",
              "            display: flex;\n",
              "            flex-direction: column;\n",
              "            gap: 20px;\n",
              "            font-family: Arial, sans-serif;\n",
              "        }\n",
              "        .comparison-box {\n",
              "            border: 2px solid #ddd;\n",
              "            border-radius: 8px;\n",
              "            padding: 15px;\n",
              "            background-color: #f9f9f9;\n",
              "        }\n",
              "        .comparison-title {\n",
              "            font-weight: bold;\n",
              "            font-size: 16px;\n",
              "            margin-bottom: 10px;\n",
              "            color: #333;\n",
              "        }\n",
              "        .zero-shot { border-left: 5px solid #FF6B6B; }\n",
              "        .structured { border-left: 5px solid #4ECDC4; }\n",
              "        .advanced { border-left: 5px solid #95E1D3; }\n",
              "    </style>\n",
              "    <div class=\"comparison-container\">\n",
              "        <div class=\"comparison-box zero-shot\">\n",
              "            <div class=\"comparison-title\">Task 2: Zero-Shot Prompt</div>\n",
              "            <div><strong>Limitations:</strong> Vague, no structure, generic content</div>\n",
              "        </div>\n",
              "        <div class=\"comparison-box structured\">\n",
              "            <div class=\"comparison-title\">Task 3: Structured Prompt</div>\n",
              "            <div><strong>Improvements:</strong> Clear structure, relevant information, better organization</div>\n",
              "        </div>\n",
              "        <div class=\"comparison-box advanced\">\n",
              "            <div class=\"comparison-title\">Task 4: Advanced Prompt Engineering</div>\n",
              "            <div><strong>Best Results:</strong> Professional tone, perfect structure, stakeholder-ready</div>\n",
              "        </div>\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Create a comparison display\n",
        "def display_comparison():\n",
        "    html_content = f\"\"\"\n",
        "    <style>\n",
        "        .comparison-container {{\n",
        "            display: flex;\n",
        "            flex-direction: column;\n",
        "            gap: 20px;\n",
        "            font-family: Arial, sans-serif;\n",
        "        }}\n",
        "        .comparison-box {{\n",
        "            border: 2px solid #ddd;\n",
        "            border-radius: 8px;\n",
        "            padding: 15px;\n",
        "            background-color: #f9f9f9;\n",
        "        }}\n",
        "        .comparison-title {{\n",
        "            font-weight: bold;\n",
        "            font-size: 16px;\n",
        "            margin-bottom: 10px;\n",
        "            color: #333;\n",
        "        }}\n",
        "        .zero-shot {{ border-left: 5px solid #FF6B6B; }}\n",
        "        .structured {{ border-left: 5px solid #4ECDC4; }}\n",
        "        .advanced {{ border-left: 5px solid #95E1D3; }}\n",
        "    </style>\n",
        "    <div class=\"comparison-container\">\n",
        "        <div class=\"comparison-box zero-shot\">\n",
        "            <div class=\"comparison-title\">Task 2: Zero-Shot Prompt</div>\n",
        "            <div><strong>Limitations:</strong> Vague, no structure, generic content</div>\n",
        "        </div>\n",
        "        <div class=\"comparison-box structured\">\n",
        "            <div class=\"comparison-title\">Task 3: Structured Prompt</div>\n",
        "            <div><strong>Improvements:</strong> Clear structure, relevant information, better organization</div>\n",
        "        </div>\n",
        "        <div class=\"comparison-box advanced\">\n",
        "            <div class=\"comparison-title\">Task 4: Advanced Prompt Engineering</div>\n",
        "            <div><strong>Best Results:</strong> Professional tone, perfect structure, stakeholder-ready</div>\n",
        "        </div>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "    display(HTML(html_content))\n",
        "\n",
        "display_comparison()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyYS9QJNmd99"
      },
      "source": [
        "---\n",
        "## Practice Exercise: Try with Input B\n",
        "\n",
        "Now it's your turn! Apply what you've learned with the narrative-style email (Input B)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWhlJw1gmd99",
        "outputId": "8179c8e1-6535-4875-aa49-a714bccc4ed4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìù PRACTICE EXERCISE: Employee Portal Revamp Project\n",
            "============================================================\n",
            "\n",
            "Generating update from narrative-style email...\n",
            "\n",
            "Generated Update:\n",
            "------------------------------------------------------------\n",
            "‚ñ∏ **Current Status**\n",
            "- UI redesign for the feedback module completed and approved by the client.\n",
            "- Ongoing testing phase.\n",
            "- Waiting for HR to confirm final features for Phase 2.\n",
            "\n",
            "‚ñ∏ **Completed Tasks**\n",
            "- Finished UI redesign for the feedback module.\n",
            "- Shared the redesigned module with the client, receiving positive feedback.\n",
            "\n",
            "‚ñ∏ **Risks/Issues**\n",
            "- Potential timeline adjustments due to pending HR feature confirmation for Phase 2.\n",
            "- API documentation requires review, though not currently blocking progress.\n",
            "\n",
            "‚ñ∏ **Next Steps**\n",
            "- Finalize API documentation review.\n",
            "- Continue with the testing phase.\n",
            "- Await HR confirmation on Phase 2 features to solidify timelines.\n",
            "- Keep the team informed of any updates regarding feature confirmation and documentation review.\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Exercise: Generate update using advanced prompt with Input B\n",
        "practice_prompt = create_advanced_prompt(input_b)\n",
        "\n",
        "print(\"üìù PRACTICE EXERCISE: Employee Portal Revamp Project\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nGenerating update from narrative-style email...\\n\")\n",
        "\n",
        "practice_response = llm.invoke(practice_prompt)\n",
        "\n",
        "print(\"Generated Update:\")\n",
        "print(\"-\"*60)\n",
        "print(practice_response)\n",
        "print(\"-\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOpCuesdmd99"
      },
      "source": [
        "---\n",
        "## Interactive Refinement Function\n",
        "\n",
        "Based on the reference implementation, we can also refine outputs iteratively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwB5WVjCmd99",
        "outputId": "257c20bb-09f3-410f-8773-ab0eaa826747"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Refinement function ready!\n",
            "\n",
            "Example usage:\n",
            "refined = refine_output(advanced_response, 'Make it more concise and add emoji bullets')\n"
          ]
        }
      ],
      "source": [
        "def refine_output(original_output, refinement_request):\n",
        "    \"\"\"\n",
        "    Refine previously generated output based on user request.\n",
        "    Similar to refine_output_with_fewshot from reference.\n",
        "    \"\"\"\n",
        "    refinement_prompt = f\"\"\"You are a project reporting assistant.\n",
        "\n",
        "Here is a project update that was previously generated:\n",
        "{original_output}\n",
        "\n",
        "Please refine this update based on the following request:\n",
        "{refinement_request}\n",
        "\n",
        "Provide the refined version:\n",
        "\"\"\"\n",
        "\n",
        "    refined_response = llm.invoke(refinement_prompt)\n",
        "    return refined_response\n",
        "\n",
        "print(\"‚úÖ Refinement function ready!\")\n",
        "print(\"\\nExample usage:\")\n",
        "print(\"refined = refine_output(advanced_response, 'Make it more concise and add emoji bullets')\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0mlemg2md99"
      },
      "source": [
        "## Try Refinement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtEwDPfWmd99"
      },
      "outputs": [],
      "source": [
        "# Example refinement\n",
        "refinement_request = \"Make the update more concise and use a numbered list instead of bullet points\"\n",
        "\n",
        "print(\"üîÑ REFINEMENT EXAMPLE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Refinement Request: {refinement_request}\\n\")\n",
        "\n",
        "refined_output = refine_output(advanced_response, refinement_request)\n",
        "\n",
        "print(\"Refined Output:\")\n",
        "print(\"-\"*60)\n",
        "print(refined_output)\n",
        "print(\"-\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVVu9dSWmd99"
      },
      "source": [
        "---\n",
        "## Summary and Key Takeaways\n",
        "\n",
        "### What You Learned:\n",
        "\n",
        "1. **Zero-Shot Prompting**: Simple direct prompts produce vague, unstructured outputs\n",
        "\n",
        "2. **Structured Prompting**: Adding task description, context, and format improves clarity\n",
        "\n",
        "3. **Advanced Prompt Engineering**: Combining role, tone, and detailed format specifications produces professional, stakeholder-ready outputs\n",
        "\n",
        "4. **Iterative Refinement**: You can further refine outputs based on specific requirements\n",
        "\n",
        "### Best Practices:\n",
        "- Define the AI's role clearly\n",
        "- Specify desired tone and format\n",
        "- Provide context and examples\n",
        "- Structure your requirements explicitly\n",
        "- Iterate and refine as needed\n",
        "\n",
        "### Technical Stack Used:\n",
        "- **Model**: IBM Granite 3.3 8B Instruct\n",
        "- **Platform**: Replicate\n",
        "- **Libraries**: langchain_community, replicate, ibm-granite-community\n",
        "- **Parameters**: Temperature=0.2, Max Tokens=1024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee_icdwSmd99"
      },
      "source": [
        "---\n",
        "## Additional Exercises\n",
        "\n",
        "Try these on your own:\n",
        "\n",
        "1. **Create your own project input** in a different format and generate an update\n",
        "\n",
        "2. **Experiment with different tones**: formal, casual, executive-level\n",
        "\n",
        "3. **Try different output formats**: paragraphs, tables, executive summaries\n",
        "\n",
        "4. **Compare temperature settings**: Try 0.0, 0.5, and 0.8 to see how creativity changes\n",
        "\n",
        "5. **Multi-project updates**: Combine multiple project inputs into a single consolidated update"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "V2_nHG1Imd9-"
      },
      "outputs": [],
      "source": [
        "# Space for your experiments\n",
        "# TODO: Add your own project input and prompts here\n",
        "\n",
        "my_project_input = \"\"\"\n",
        "# Add your project information here\n",
        "\"\"\"\n",
        "\n",
        "# Your custom prompt\n",
        "my_prompt = \"\"\"\n",
        "# Create your prompt here\n",
        "\"\"\"\n",
        "\n",
        "# Uncomment to run:\n",
        "# my_response = llm.invoke(my_prompt)\n",
        "# print(my_response)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}