{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/niit-ibm/lt4-pe-lab1/blob/main/pe_lab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "743I23kvmd95"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install git+https://github.com/ibm-granite-community/utils \\\n",
        "    \"langchain_community<0.3.0\" \\\n",
        "    replicate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17ZcM1y-md96"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import replicate\n",
        "from langchain_community.llms import Replicate\n",
        "from ibm_granite_community.notebook_utils import get_env_var\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown, HTML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gUVkPNbmd97"
      },
      "outputs": [],
      "source": [
        "# Use the utility function to get from environment or prompt\n",
        "replicate_api_token = get_env_var(\"REPLICATE_API_TOKEN\")\n",
        "os.environ[\"REPLICATE_API_TOKEN\"] = replicate_api_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77Z9KDjrmd97"
      },
      "outputs": [],
      "source": [
        "# Model configuration - exactly as in reference\n",
        "MODEL_NAME = \"ibm-granite/granite-3.3-8b-instruct\"\n",
        "MAX_TOKENS = 1024\n",
        "TEMPERATURE = 0.2\n",
        "\n",
        "# Initialize the model\n",
        "llm = Replicate(\n",
        "    model=MODEL_NAME,\n",
        "    model_kwargs={\n",
        "        \"max_tokens\": MAX_TOKENS,\n",
        "        \"temperature\": TEMPERATURE\n",
        "    }\n",
        ")\n",
        "\n",
        "print(f\"âœ… Model initialized: {MODEL_NAME}\")\n",
        "print(f\"   Max Tokens: {MAX_TOKENS}\")\n",
        "print(f\"   Temperature: {TEMPERATURE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqU1-2TImd98"
      },
      "outputs": [],
      "source": [
        "# Input To Project: Bulleted, messy, partial notes\n",
        "input = \"\"\"Project: Alpha CRM Upgrade â€” Weekly Update Notes\n",
        "â€¢ Login issue fixed for 80% users, pending bug #3421\n",
        "â€¢ Deployment for Phase 2 pushed from 15th to 19th\n",
        "â€¢ Client appreciated responsiveness\n",
        "â€¢ Risk: insufficient test data for UAT\n",
        "â€¢ Next steps: Dev team to finalize API changes, Testing team to complete regression\n",
        "â€¢ Need to highlight dependency on Data Engineering\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "print(\"Sample inputs loaded successfully!\")\n",
        "print(\"\\nYou will use 'input' as the Project Updates in the exercises below.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxf2TH3Cmd98"
      },
      "outputs": [],
      "source": [
        "# Direct, zero-shot prompt\n",
        "zero_shot_prompt = \"Write a 120-word weekly project update.\"\n",
        "\n",
        "print(\"ðŸ”µ TASK 2: Zero-Shot Prompting\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Prompt: {zero_shot_prompt}\\n\")\n",
        "\n",
        "# Get response from model\n",
        "zero_shot_response = llm.invoke(zero_shot_prompt)\n",
        "\n",
        "print(\"Response:\")\n",
        "print(\"-\"*60)\n",
        "print(zero_shot_response)\n",
        "print(\"-\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpbMvwv7md99"
      },
      "outputs": [],
      "source": [
        "# Task-specific prompt with structure\n",
        "def create_structured_prompt(project_input):\n",
        "    prompt = f\"\"\"Create a 120-word weekly project update for internal stakeholders.\n",
        "Include: current status, completed tasks, risks, and next steps.\n",
        "\n",
        "Based on this input:\n",
        "{project_input}\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n",
        "# Let's use Input A for this task\n",
        "structured_prompt = create_structured_prompt(input)\n",
        "\n",
        "print(\"ðŸŸ¢ TASK 3: Task-Specific Prompt with Structured Instructions\")\n",
        "print(\"=\"*60)\n",
        "print(\"Prompt:\")\n",
        "print(structured_prompt)\n",
        "print(\"\\nGenerating response...\\n\")\n",
        "\n",
        "# Get response from model\n",
        "structured_response = llm.invoke(structured_prompt)\n",
        "\n",
        "print(\"Response:\")\n",
        "print(\"-\"*60)\n",
        "print(structured_response)\n",
        "print(\"-\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjfcZa9Gmd99"
      },
      "outputs": [],
      "source": [
        "# Advanced prompt with role, tone, and format\n",
        "def create_advanced_prompt(project_input):\n",
        "    prompt = f\"\"\"You are a project reporting assistant for an IT services company.\n",
        "\n",
        "Task: Generate a 120-word weekly project update for internal stakeholders.\n",
        "\n",
        "Tone: Concise and professional.\n",
        "\n",
        "Format: Use bullet points for the following sections:\n",
        "- Current Status\n",
        "- Completed Tasks\n",
        "- Risks/Issues\n",
        "- Next Steps\n",
        "\n",
        "Input information:\n",
        "{project_input}\n",
        "\n",
        "Generate the update now:\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n",
        "# Test with Project Input\n",
        "advanced_prompt = create_advanced_prompt(input)\n",
        "\n",
        "print(\"ðŸŸ£ TASK 4: Advanced Prompt Engineering\")\n",
        "print(\"=\"*60)\n",
        "print(\"Prompt:\")\n",
        "print(advanced_prompt)\n",
        "print(\"\\nGenerating response...\\n\")\n",
        "\n",
        "# Get response from model\n",
        "advanced_response = llm.invoke(advanced_prompt)\n",
        "\n",
        "print(\"Response:\")\n",
        "print(\"-\"*60)\n",
        "print(advanced_response)\n",
        "print(\"-\"*60)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}